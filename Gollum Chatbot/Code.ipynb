{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE1724 Project - Gollum Chatbot\n",
    "\n",
    "This chatbot attempts to impersonate the character Gollum/Smeagol from Lord of the Rings. Originally I was planning on using data from the books, but then switched to movie scripts as they were more easy to acquire lines from. The dataset is from https://www.kaggle.com/datasets/paultimothymooney/lord-of-the-rings-data. Some inspiration, such as the usage of Microsoft's DialoGPT model, was taken from a Rick and Morty bot that I saw online but can no longer find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "#other normal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "#pytorch stuff\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "#transformers imports\n",
    "from transformers import (AdamW, AutoConfig, AutoModelForCausalLM, AutoTokenizer,\n",
    "                          MODEL_WITH_LM_HEAD_MAPPING, PreTrainedModel,\n",
    "                          PreTrainedTokenizer, WEIGHTS_NAME,\n",
    "                          get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration\n",
    "\n",
    "The arguments below were found online, and some, such as learning rate, were varied to see different results. Most of the arguments are pretty self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurations and logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "#arguments class for configuration\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = self.model_name_or_path\n",
    "        self.tokenizer_name = self.model_name_or_path\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.train_batch_size = 1\n",
    "        self.eval_batch_size = 1 \n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 5\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Aside from seperating Gollum's/Smeagol's lines from the rest of the data, I'll have to use several prior responses for each dialogue line for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "script = pd.read_excel(\"my_scripts2.xlsx\")\n",
    "\n",
    "#number of context lines\n",
    "n = 7\n",
    "\n",
    "#list for contexted dialogue\n",
    "contexted = []\n",
    "\n",
    "#loop trough data starting from nth entry\n",
    "for i in range(n, len(script['dialog']), 2):\n",
    "    #get n preceeding dialogues as context and current as response\n",
    "    row = [script['dialog'][j] for j in range(i - n, i)] + [script['dialog'][i]]\n",
    "    #append row to contexted data\n",
    "    contexted.append(row)\n",
    "\n",
    "#dataframe from contexted list\n",
    "df = pd.DataFrame(contexted, columns=['context/' + str(i) for i in range(n)] + ['response'])\n",
    "df.head(5)\n",
    "\n",
    "#split data\n",
    "train_set, val_set = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is going to be converted into a better format for the model. Each response will be concatenated in one string for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to construct conversation sequences\n",
    "def construct_conv(row, tokenizer, eos=True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    tokens = tokenizer.encode(row + (tokenizer.eos_token if eos else ''), return_tensors='pt')\n",
    "    return tokens.squeeze().tolist()\n",
    "\n",
    "#dataset class for handling conversations\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "            self.examples = []\n",
    "            for _, row_series in df.iterrows():\n",
    "                row_series_str = row_series.apply(lambda x: '' if pd.isna(x) else str(x))\n",
    "                row_text = ' '.join(row_series_str[:-1])\n",
    "                response_text = str(row_series.iloc[-1]) if not pd.isna(row_series.iloc[-1]) else ''\n",
    "                conv_tokens = construct_conv(row_text + ' ' + response_text, tokenizer)\n",
    "                self.examples.append(conv_tokens)\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)\n",
    "\n",
    "#function for loading and caching examples\n",
    "def load_and_cache_examples(args, tokenizer, df_train, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_train)\n",
    "\n",
    "#function to set the random seed for reproducibility\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "#function to sort checkpoints\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False):\n",
    "    ordering_and_checkpoint_path = []\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(f\".*{checkpoint_prefix}-([0-9]+)\", path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "#function to manage checkpoint rotation\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False):\n",
    "    if not args.save_total_limit or args.save_total_limit <= 0:\n",
    "        return\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [%s] due to args.save_total_limit\", checkpoint)\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    #initialize TensorBoard if master process\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    #calculate effective batch size\n",
    "    args.train_batch_size = args.train_batch_size\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    #select appropriate sampler\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last=True)\n",
    "\n",
    "    #total number of training steps\n",
    "    t_total = args.max_steps if args.max_steps > 0 else len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    #prepare model for training\n",
    "    model = model.module if hasattr(model, \"module\") else model  #handling distributed training scenario\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #prepare optimizer and scheduler\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "    #load optimizer and scheduler if resuming training\n",
    "    if args.model_name_or_path and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\")):\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"Batch size = {args.train_batch_size}\")\n",
    "    logger.info(f\"Total optimization steps = {t_total}\")\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "\n",
    "    set_seed(args)  #here for reproducibility\n",
    "\n",
    "    for _ in trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]):\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])):\n",
    "            model.train()\n",
    "            inputs, labels = batch, batch\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  #update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                #log metrics\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                #save model checkpoint\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    output_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
    "\n",
    "                    _rotate_checkpoints(args, \"checkpoint\")\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(f\"Saving optimizer and scheduler states to {output_dir}\")\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, df_train, df_val, prefix=\"\"):\n",
    "    eval_output_dir = args.output_dir\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_train, df_val, evaluate=True)\n",
    "\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)  #ensure directory exists for outputs\n",
    "    args.eval_batch_size = args.eval_batch_size  \n",
    "\n",
    "    #collate function to handle padding\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    #setup DataLoader for evaluation\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last=True)\n",
    "\n",
    "    logger.info(f\"***** Running evaluation {prefix} *****\")\n",
    "    logger.info(f\"Num examples = {len(eval_dataset)}\")\n",
    "    logger.info(f\"Batch size = {args.eval_batch_size}\")\n",
    "    \n",
    "    eval_loss, nb_eval_steps = 0.0, 0  #initialize evaluation loss and step counter\n",
    "    model.eval()  #set model to evaluation mode\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = batch, batch  #unpack batch\n",
    "        inputs = inputs.to(args.device)  #move inputs to device\n",
    "        labels = labels.to(args.device)  #move labels to device\n",
    "\n",
    "        with torch.no_grad():  #disable gradient calculation for evaluation\n",
    "            outputs = model(inputs, labels=labels)  #forward pass\n",
    "            lm_loss = outputs[0]  #extract loss\n",
    "            eval_loss += lm_loss.mean().item()  #accumulate loss\n",
    "        nb_eval_steps += 1  #increment step counter\n",
    "\n",
    "    eval_loss /= nb_eval_steps  #calculate average loss\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))  #calculate perplexity\n",
    "\n",
    "    result = {\"perplexity\": perplexity}  #prepare results\n",
    "\n",
    "    #log evaluation results\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(f\"***** Eval results {prefix} *****\")\n",
    "        for key, value in result.items():\n",
    "            logger.info(f\"{key} = {value}\")\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_trn, df_val):\n",
    "    args = Args()  #initialize configuration arguments\n",
    "\n",
    "    #check for continuation from checkpoint\n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if not sorted_checkpoints:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint found in output_dir.\")\n",
    "        args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    #ensure output directory is ready\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir and not args.should_continue:\n",
    "        raise ValueError(f\"Output directory ({args.output_dir}) exists and is not empty. Use --overwrite_output_dir.\")\n",
    "\n",
    "    device = torch.device(\"cpu\")  #setup device for CPU training\n",
    "    args.n_gpu = 0\n",
    "    args.device = device\n",
    "\n",
    "    #setup logging\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "    logger.warning(f\"Process rank: {args.local_rank}, device: {device}, n_gpu: {args.n_gpu}, distributed training: {bool(args.local_rank != -1)}\")\n",
    "\n",
    "    set_seed(args)  #set random seed for reproducibility\n",
    "\n",
    "    #initialize model and tokenizer\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
    "    model.to(args.device)\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    #begin training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(f\"global_step = {global_step}, average loss = {tr_loss}\")\n",
    "\n",
    "    #save trained model and tokenizer\n",
    "    if args.do_train:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        logger.info(f\"Saving model checkpoint to {args.output_dir}\")\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model  #handle parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    #begin evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir] if not args.eval_all_checkpoints else list(os.path.dirname(c) for c in sorted(glob.glob(f\"{args.output_dir}/**/{WEIGHTS_NAME}\", recursive=True)))\n",
    "        logger.info(f\"Evaluate the following checkpoints: {checkpoints}\")\n",
    "        for checkpoint in checkpoints:\n",
    "            prefix = checkpoint.split(\"/\")[-1] if \"checkpoint\" in checkpoint else \"\"\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            results.update({f\"{k}_{prefix}\": v for k, v in result.items()})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model can be trained on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 13:33:14 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x0000021083DACDD0>\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "C:\\Users\\Seb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   ***** Running training *****\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Num examples = 103\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Num Epochs = 5\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Batch size = 1\n",
      "04/01/2024 13:33:15 - INFO - __main__ -   Total optimization steps = 515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8cdaae458f4154baa1d6d4b2ddde7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff8e4bb5fad475298c697fadc29410c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34953b67a42a490e90170bffbf4511d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528c6380f1684dd3b389116351f5b076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c33bcddbab54e4d9ae02d1a409718e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7963f5f60b07455fb2fdb2b8d433f5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 13:45:06 - INFO - __main__ -   global_step = 515, average loss = 2.5333530737358387\n",
      "04/01/2024 13:45:06 - INFO - __main__ -   Saving model checkpoint to output-small\n",
      "04/01/2024 13:45:07 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small']\n",
      "04/01/2024 13:45:07 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "04/01/2024 13:45:07 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "04/01/2024 13:45:07 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "04/01/2024 13:45:07 - INFO - __main__ -   Num examples = 12\n",
      "04/01/2024 13:45:07 - INFO - __main__ -   Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24d78ceaaa34dc2be6c8e969c5117f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 13:45:11 - INFO - __main__ -   ***** Eval results  *****\n",
      "04/01/2024 13:45:11 - INFO - __main__ -   perplexity = 5.865156173706055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(5.8652)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(train_set, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to stop)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gollum: The last hobbit, yes. Now, if only I had a master's in drawing, I wouldn't have to destroy precious. I just want to collect the precious. What's your idea of a perfect meal? A fat, juicy fish, caught fresh from the river, yes, precious. Do you like the cold? Not too cold, no. Smeagol! Sneaky little fish, yes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gollum: Sneak. We likes it, sss. But we needs to riddle ourselves, yes? The precious, yes’s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gollum: ’talks to precious’’, smeagols, yes, talks to precious.’es, sakes with fishes, yes.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelForCausalLM.from_pretrained('output-small')\n",
    "model.to(torch.device(\"cpu\"))\n",
    "\n",
    "#left padding\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "#initialize chat history variable\n",
    "chat_history_ids = None\n",
    "\n",
    "#chat \n",
    "print(\"Let's chat! (type 'quit' to stop)\")\n",
    "while True:\n",
    "    user_input = input(\">> User: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    #encode new user input, adding EOS token\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    #manages length\n",
    "    if chat_history_ids is not None:\n",
    "        #trim chat_history_ids if needed\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "        #make sure input does not exceed the max_length for the model\n",
    "        max_input_length = model.config.n_positions\n",
    "        if bot_input_ids.size(-1) > max_input_length:\n",
    "            bot_input_ids = bot_input_ids[:, -max_input_length:]\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "\n",
    "    #response\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=500,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=20,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    #print the bot's response\n",
    "    bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\"Gollum: {bot_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
